{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b843de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "import json\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7f1786",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5b787a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "HAM_CSV = \"raw/data_v1.csv\"\n",
    "WOHAM_CSV = \"raw/data_notagword_v1.csv\"\n",
    "\n",
    "VAL_FROM_TRAIN_RATIO = 0.10\n",
    "\n",
    "X_COLUMN = \"text\"\n",
    "TOKEN_TAG_COLUMN = \"tag_by_word\"\n",
    "Y_COLUMN = \"tags\"\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980b890e",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09c42d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4141\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tag_by_word</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[ประเทศจีน, ชอบ, ปลอม, เอา, ยาง, รถ, ไป, ทำ, ไ...</td>\n",
       "      <td>[I-Influencer, Fb-Refer, Fb-Refer, Fb-Refer, F...</td>\n",
       "      <td>Fake News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ประกาศ, กฎอัยการศึก, ปิด, การ, เข้า, ออก, ทุก...</td>\n",
       "      <td>[T-Clickbait, T-Clickbait, , , , , , , , , , ,...</td>\n",
       "      <td>Undefined</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ประโยชน์, มาก, ก็, ใช่, ว่า, จะ, ไม่มี, โทษ, ...</td>\n",
       "      <td>[, , , , , , , , , , M-Convincing, M-Convincin...</td>\n",
       "      <td>Undefined</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[มั่นใจ, กระซิบหูหนู, พระพิฆเนศ, ไม่ติด, เชื้อ...</td>\n",
       "      <td>[T-Clickbait, T-Clickbait, T-Clickbait, T-Clic...</td>\n",
       "      <td>Fake News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[พวก, ที่, เชื่อ, ว่า, คน, ไม่, ป่วย, ไม่, ต้อ...</td>\n",
       "      <td>[, , , , , , , , , , , , , , , , , , , , , , ,...</td>\n",
       "      <td>Undefined</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  [ประเทศจีน, ชอบ, ปลอม, เอา, ยาง, รถ, ไป, ทำ, ไ...   \n",
       "1  [ประกาศ, กฎอัยการศึก, ปิด, การ, เข้า, ออก, ทุก...   \n",
       "2  [ประโยชน์, มาก, ก็, ใช่, ว่า, จะ, ไม่มี, โทษ, ...   \n",
       "3  [มั่นใจ, กระซิบหูหนู, พระพิฆเนศ, ไม่ติด, เชื้อ...   \n",
       "4  [พวก, ที่, เชื่อ, ว่า, คน, ไม่, ป่วย, ไม่, ต้อ...   \n",
       "\n",
       "                                         tag_by_word       tags  \n",
       "0  [I-Influencer, Fb-Refer, Fb-Refer, Fb-Refer, F...  Fake News  \n",
       "1  [T-Clickbait, T-Clickbait, , , , , , , , , , ,...  Undefined  \n",
       "2  [, , , , , , , , , , M-Convincing, M-Convincin...  Undefined  \n",
       "3  [T-Clickbait, T-Clickbait, T-Clickbait, T-Clic...  Fake News  \n",
       "4  [, , , , , , , , , , , , , , , , , , , , , , ,...  Undefined  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_ham_converters = {\n",
    "    \"text\": literal_eval,\n",
    "    \"tag_by_word\": literal_eval\n",
    "}\n",
    "\n",
    "w_ham_df = pd.read_csv(HAM_CSV, converters=w_ham_converters)\n",
    "print(len(w_ham_df))\n",
    "w_ham_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0c51b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2998\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[พี่, สาว, คน, ข้าง, บ้าน, เป็น, มะเร็ง, ต่อม,...</td>\n",
       "      <td>Fake News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[กิน, แทน, ข้าว, หนุ่ม, เวียดนาม, ปวด, ท้อง, 2...</td>\n",
       "      <td>Fake News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ผง, ชูรส, นี่, ก็, สาร, พิษ, ก่อ, เกิด, โรค, ...</td>\n",
       "      <td>Fake News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[รู้, ไว้, มี, ประโยชน์, !, , , ไม่, อยาก, เป็...</td>\n",
       "      <td>Fake News</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[มะหวด, , , (, หมากหวด, ข่า, ), , , ผลไม้, ป่า...</td>\n",
       "      <td>Fake News</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       tags\n",
       "0  [พี่, สาว, คน, ข้าง, บ้าน, เป็น, มะเร็ง, ต่อม,...  Fake News\n",
       "1  [กิน, แทน, ข้าว, หนุ่ม, เวียดนาม, ปวด, ท้อง, 2...  Fake News\n",
       "2  [ผง, ชูรส, นี่, ก็, สาร, พิษ, ก่อ, เกิด, โรค, ...  Fake News\n",
       "3  [รู้, ไว้, มี, ประโยชน์, !, , , ไม่, อยาก, เป็...  Fake News\n",
       "4  [มะหวด, , , (, หมากหวด, ข่า, ), , , ผลไม้, ป่า...  Fake News"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wo_ham_converters = {\n",
    "    \"text\": lambda x: literal_eval(x)\n",
    "}\n",
    "wo_ham_df = pd.read_csv(WOHAM_CSV, converters=wo_ham_converters)\n",
    "print(len(wo_ham_df))\n",
    "\n",
    "wo_ham_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfc2ed58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sampling: 7139\n"
     ]
    }
   ],
   "source": [
    "print(\"Total sampling: %d\"%(len(w_ham_df) + len(wo_ham_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c09b75",
   "metadata": {},
   "source": [
    "## Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc453ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = wo_ham_df[X_COLUMN], wo_ham_df[Y_COLUMN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfb5d04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2698 2698\n",
      "300 300\n"
     ]
    }
   ],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    X, Y, test_size=VAL_FROM_TRAIN_RATIO,\n",
    "    stratify=Y, random_state=RANDOM_SEED\n",
    ")\n",
    "print(len(x_train), len(y_train))\n",
    "print(len(x_val), len(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a63d5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4141 4141 4141\n"
     ]
    }
   ],
   "source": [
    "x_test, x_tags_test, y_test = w_ham_df[X_COLUMN], w_ham_df[TOKEN_TAG_COLUMN], w_ham_df[Y_COLUMN]\n",
    "print(len(x_test), len(x_tags_test), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e29996e",
   "metadata": {},
   "source": [
    "### Remove train/test overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0534c11",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# leaks: 1376\n"
     ]
    }
   ],
   "source": [
    "train_texts = [\"\".join(x) for x in X]\n",
    "\n",
    "nondup_x_test = []\n",
    "nondup_x_tags_test = []\n",
    "nondup_y_test = []\n",
    "\n",
    "n_leak = 0\n",
    "for x, tags, y in zip(x_test, x_tags_test, y_test):\n",
    "    if \"\".join(x) in train_texts:\n",
    "        n_leak += 1\n",
    "        # print(n_leak, \"\".join(x))\n",
    "        continue\n",
    "    nondup_x_test.append(x)\n",
    "    nondup_x_tags_test.append(tags)\n",
    "    nondup_y_test.append(y)\n",
    "    \n",
    "print(\"# leaks: %d\"%n_leak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7693e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2765 2765 2765\n"
     ]
    }
   ],
   "source": [
    "print(len(nondup_x_test), len(nondup_x_tags_test), len(nondup_y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dee9205",
   "metadata": {},
   "source": [
    "## Data saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ad99713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_jsonl(\n",
    "    _path: str,\n",
    "    _X: List[str],\n",
    "    _Y: List[str],\n",
    "    _X_TAGS: List[str] = None\n",
    ") -> None:\n",
    "    samples = []\n",
    "    if _X_TAGS is not None:\n",
    "        for x, x_tags, y in zip(_X, _X_TAGS, _Y):\n",
    "            samples.append({\"Text\": x, \"Token Tags\": x_tags, \"Document Tag\": y})\n",
    "    else:\n",
    "        for x, y in zip(_X, _Y):\n",
    "            samples.append({\"Text\": x, \"Document Tag\": y})\n",
    "            \n",
    "    f = open(_path, 'w')\n",
    "    for sample in samples:\n",
    "        f.write(\"%s\\n\" % json.dumps(sample, ensure_ascii=False))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9d3fef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_jsonl(\n",
    "    \"train_v1.jsonl\",\n",
    "    x_train, y_train\n",
    ")\n",
    "\n",
    "to_jsonl(\n",
    "    \"val_v1.jsonl\",\n",
    "    x_val, y_val\n",
    ")\n",
    "\n",
    "to_jsonl(\n",
    "    \"test_v1.jsonl\",\n",
    "    nondup_x_test, nondup_y_test, nondup_x_tags_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1154d15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
